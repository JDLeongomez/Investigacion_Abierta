
@article{albersWhenPowerAnalyses2018,
  title = {When Power Analyses Based on Pilot Data Are Biased: {{Inaccurate}} Effect Size Estimators and Follow-up Bias},
  shorttitle = {When Power Analyses Based on Pilot Data Are Biased},
  author = {Albers, Casper and Lakens, Dani{\"e}l},
  year = {2018},
  month = jan,
  volume = {74},
  pages = {187--195},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2017.09.004},
  abstract = {When designing a study, the planned sample size is often based on power analyses. One way to choose an effect size for power analyses is by relying on pilot data. A-priori power analyses are only accurate when the effect size estimate is accurate. In this paper we highlight two sources of bias when performing a-priori power analyses for between-subject designs based on pilot data. First, we examine how the choice of the effect size index ({$\eta$}2, {$\omega$}2 and {$\epsilon$}2) affects the sample size and power of the main study. Based on our observations, we recommend against the use of {$\eta$}2 in a-priori power analyses. Second, we examine how the maximum sample size researchers are willing to collect in a main study (e.g. due to time or financial constraints) leads to overestimated effect size estimates in the studies that are performed. Determining the required sample size exclusively based on the effect size estimates from pilot data, and following up on pilot studies only when the sample size estimate for the main study is considered feasible, creates what we term follow-up bias. We explain how follow-up bias leads to underpowered main studies. Our simulations show that designing main studies based on effect sizes estimated from small pilot studies does not yield desired levels of power due to accuracy bias and follow-up bias, even when publication bias is not an issue. We urge researchers to consider alternative approaches to determining the sample size of their studies, and discuss several options.},
  file = {G\:\\Mi unidad\\Zotero\\Journal of Experimental Social Psychology\\Albers_Lakens_2018_When power analyses based on pilot data are biased.pdf;D\:\\Zotero Library\\storage\\CXD272UJ\\S002210311630230X.html},
  journal = {Journal of Experimental Social Psychology},
  keywords = {Effect size,Epsilon-squared,Eta-squared,Follow-up bias,Omega-squared,Power analysis},
  language = {en}
}

@article{baker500ScientistsLift2016,
  title = {1,500 Scientists Lift the Lid on Reproducibility},
  author = {Baker, Monya},
  year = {2016},
  month = may,
  volume = {533},
  pages = {452},
  doi = {10.1038/533452a},
  abstract = {Survey sheds light on the `crisis' rocking research.},
  file = {G\:\\Mi unidad\\Zotero\\Nature News\\Baker_2016_1,500 scientists lift the lid on reproducibility.pdf;D\:\\Zotero Library\\storage\\BEKYRSFU\\1-500-scientists-lift-the-lid-on-reproducibility-1.html},
  journal = {Nature News},
  language = {en},
  number = {7604}
}

@article{bakkerSmallMediumLarge2019,
  title = {Beyond Small, Medium, or Large: Points of Consideration When Interpreting Effect Sizes},
  shorttitle = {Beyond Small, Medium, or Large},
  author = {Bakker, Arthur and Cai, Jinfa and English, Lyn and Kaiser, Gabriele and Mesa, Vilma and Van Dooren, Wim},
  year = {2019},
  month = sep,
  volume = {102},
  pages = {1--8},
  issn = {1573-0816},
  doi = {10.1007/s10649-019-09908-4},
  file = {G\:\\Mi unidad\\Zotero\\Educational Studies in Mathematics\\Bakker et al_2019_Beyond small, medium, or large.pdf},
  journal = {Educational Studies in Mathematics},
  language = {en},
  number = {1}
}

@article{benjaminRedefineStatisticalSignificance2018,
  title = {Redefine Statistical Significance},
  author = {Benjamin, Daniel J. and Berger, James O. and Johannesson, Magnus and Nosek, Brian A. and Wagenmakers, E.-J. and Berk, Richard and Bollen, Kenneth A. and Brembs, Bj{\"o}rn and Brown, Lawrence and Camerer, Colin and Cesarini, David and Chambers, Christopher D. and Clyde, Merlise and Cook, Thomas D. and De Boeck, Paul and Dienes, Zoltan and Dreber, Anna and Easwaran, Kenny and Efferson, Charles and Fehr, Ernst and Fidler, Fiona and Field, Andy P. and Forster, Malcolm and George, Edward I. and Gonzalez, Richard and Goodman, Steven and Green, Edwin and Green, Donald P. and Greenwald, Anthony G. and Hadfield, Jarrod D. and Hedges, Larry V. and Held, Leonhard and Hua Ho, Teck and Hoijtink, Herbert and Hruschka, Daniel J. and Imai, Kosuke and Imbens, Guido and Ioannidis, John P. A. and Jeon, Minjeong and Jones, James Holland and Kirchler, Michael and Laibson, David and List, John and Little, Roderick and Lupia, Arthur and Machery, Edouard and Maxwell, Scott E. and McCarthy, Michael and Moore, Don A. and Morgan, Stephen L. and Munaf{\'o}, Marcus and Nakagawa, Shinichi and Nyhan, Brendan and Parker, Timothy H. and Pericchi, Luis and Perugini, Marco and Rouder, Jeff and Rousseau, Judith and Savalei, Victoria and Sch{\"o}nbrodt, Felix D. and Sellke, Thomas and Sinclair, Betsy and Tingley, Dustin and Van Zandt, Trisha and Vazire, Simine and Watts, Duncan J. and Winship, Christopher and Wolpert, Robert L. and Xie, Yu and Young, Cristobal and Zinman, Jonathan and Johnson, Valen E.},
  year = {2018},
  month = jan,
  volume = {2},
  pages = {6--10},
  publisher = {{Nature Publishing Group; http://web.archive.org/web/20200411231745/https://www.nature.com/articles/s41562-017-0189-z}},
  issn = {2397-3374},
  doi = {10.1038/s41562-017-0189-z},
  abstract = {We propose to change the default P-value threshold for statistical significance from 0.05 to 0.005 for claims of new discoveries.},
  copyright = {2017 The Author(s)},
  file = {G\:\\Mi unidad\\Zotero\\Nature Human Behaviour\\Benjamin et al_2018_Redefine statistical significance.pdf;D\:\\Zotero Library\\storage\\U87LQB75\\s41562-017-0189-z.html},
  journal = {Nature Human Behaviour},
  language = {en},
  number = {1}
}

@article{blakesleyComparisonsMethodsMultiple2009,
  title = {Comparisons of Methods for Multiple Hypothesis Testing in Neuropsychological Research},
  author = {Blakesley, Richard E. and Mazumdar, Sati and Dew, Mary Amanda and Houck, Patricia R. and Tang, Gong and Reynolds III, Charles F. and Butters, Meryl A.},
  year = {2009},
  volume = {23},
  pages = {255--264},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1931-1559(Electronic),0894-4105(Print)},
  doi = {10.1037/a0012850},
  abstract = {Hypothesis testing with multiple outcomes requires adjustments to control Type I error inflation, which reduces power to detect significant differences. Maintaining the prechosen Type I error level is challenging when outcomes are correlated. This problem concerns many research areas, including neuropsychological research in which multiple, interrelated assessment measures are common. Standard p value adjustment methods include Bonferroni-, Sidak-, and resampling-class methods. In this report, the authors aimed to develop a multiple hypothesis testing strategy to maximize power while controlling Type I error. The authors conducted a sensitivity analysis, using a neuropsychological dataset, to offer a relative comparison of the methods and a simulation study to compare the robustness of the methods with respect to varying patterns and magnitudes of correlation between outcomes. The results lead them to recommend the Hochberg and Hommel methods (step-up modifications of the Bonferroni method) for mildly correlated outcomes and the step-down minP method (a resampling-based method) for highly correlated outcomes. The authors note caveats regarding the implementation of these methods using available software. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {G\:\\Mi unidad\\Zotero\\Neuropsychology\\Blakesley et al_2009_Comparisons of methods for multiple hypothesis testing in neuropsychological.pdf;D\:\\Zotero Library\\storage\\JH9D65V7\\2009-02621-013.html},
  journal = {Neuropsychology},
  keywords = {Error Analysis,Hypothesis Testing,Test Performance,Testing,Type I Errors},
  number = {2}
}

@misc{bockWhatReplicationCrisis2018,
  title = {What Is the {{Replication Crisis}}?},
  author = {Bock, Tim},
  year = {2018},
  month = aug,
  abstract = {The replication crisis is the growing belief that many scientific studies are unable to be reproduced. This could imply that significant theories are wrong.},
  file = {D\:\\Zotero Library\\storage\\BKD95J4D\\what-is-the-replication-crisis.html},
  journal = {Displayr},
  language = {en-US}
}

@article{bonferroniTeoriaStatisticaClassi1936,
  title = {Teoria Statistica Delle Classi e Calcolo Delle Probabilit{\`a}},
  author = {Bonferroni, C. E.},
  year = {1936},
  journal = {Pubblicazioni del R Istituto Superiore di Scienze Economiche e Commerciali di Firenze}
}

@article{buttonPowerFailureWhy2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = {2013},
  month = may,
  volume = {14},
  pages = {365--376},
  publisher = {{Nature Publishing Group; http://web.archive.org/web/20200411225421/https://www.nature.com/articles/nrn3475}},
  issn = {1471-0048},
  doi = {10.1038/nrn3475},
  abstract = {Low-powered studies lead to overestimates of effect size and low reproducibility of results. In this Analysis article, Munaf{\`o} and colleagues show that the average statistical power of studies in the neurosciences is very low, discuss ethical implications of low-powered studies and provide recommendations to improve research practices.},
  copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  file = {G\:\\Mi unidad\\Zotero\\Nature Reviews Neuroscience\\Button et al_2013_Power failure.pdf;D\:\\Zotero Library\\storage\\9VB2Y52M\\nrn3475.html},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {5}
}

@misc{caldwellPowerAnalysisSuperpower2020,
  title = {Power {{Analysis}} with {{Superpower}}},
  author = {Caldwell, Aaron R. and Lakens, Dani{\"e}l},
  year = {2020},
  abstract = {This is a book describing the capabilities of the Superpower R package.},
  file = {D\:\\Zotero Library\\storage\\VGNL7WJQ\\SuperpowerBook.html},
  howpublished = {https://aaroncaldwell.us/SuperpowerBook/},
  note = {https://aaroncaldwell.us/SuperpowerBook/}
}

@misc{caldwellSuperpowerSimulationBasedPower2020,
  title = {Superpower: {{Simulation}}-{{Based Power Analysis}} for {{Factorial Designs}}},
  shorttitle = {Superpower},
  author = {Caldwell, Aaron R. and Lakens, Dani{\"e}l and DeBruine, Lisa and Love, Jonathon},
  year = {2020},
  month = feb,
  abstract = {Functions to perform simulations of ANOVA designs of up to three factors. Calculates the observed power and average observed effect size for all main effects and interactions in the ANOVA, and all simple comparisons between conditions. Includes functions for analytic power calculations and additional helper functions that compute effect sizes for ANOVA designs, observed error rates in the simulations, and functions to plot power curves. Please see Lakens, D., \& Caldwell, A. R. (2019). "Simulation-Based Power-Analysis for Factorial ANOVA Designs". {$<$}doi:10.31234/osf.io/baxsf{$>$}.},
  copyright = {MIT + file LICENSE},
  note = {http://web.archive.org/web/20200417044214/https://cran.r-project.org/web/packages/Superpower/index.html}
}

@misc{champelyPwrBasicFunctions2020,
  title = {Pwr: {{Basic Functions}} for {{Power Analysis}}},
  shorttitle = {Pwr},
  author = {Champely, Stephane and Ekstrom, Claus and Dalgaard, Peter and Gill, Jeffrey and Weibelzahl, Stephan and Anandkumar, Aditya and Ford, Clay and Volcic, Robert and Rosario, Helios De},
  year = {2020},
  month = mar,
  abstract = {Power analysis functions along the lines of Cohen (1988).},
  copyright = {GPL ({$\geq$} 3)},
  keywords = {ClinicalTrials}
}

@book{chathamPlannedContrastsOverview1999,
  title = {Planned {{Contrasts}}: {{An Overview}} of {{Comparison Methods}}},
  shorttitle = {Planned {{Contrasts}}},
  author = {Chatham, Kathy},
  year = {1999},
  month = jan,
  abstract = {Contrasts or comparisons can be used to investigate specific differences between means. Contrasts, as explained by B. Thompson (1985, 1994) are coding vectors that mathematically express hypotheses. The most basic categories of contrasts are planned and unplanned. The purpose of this paper is to explain the relative advantages of using planned contrasts rather than unplanned contrasts and to illustrate several different planned contrasts that are available. Planned contrasts are designed to test predetermined specific hypotheses and have more statistical power against Type II errors than unplanned comparisons. The most common distinction among planned contrasts is whether they are orthogonal (uncorrelated) or nonorthogonal. A hypothetical data set illustrates the difference  between the two. Descriptions are also given of more specific coding methods for contrasts. These methods include trend versus nontrend analyses, dummy coding, and effect coding. The use of planned contrasts is often a good alternative to traditional analysis of variance. (Contains 4 tables and 18 references.) (SLD)},
  file = {G\:\\Mi unidad\\Zotero\\undefined\\Chatham_1999_Planned Contrasts.pdf;D\:\\Zotero Library\\storage\\APZK38AH\\eric.ed.gov.html},
  keywords = {Coding,Comparative Analysis,Correlation,Hypothesis Testing,Power (Statistics),Research Methodology},
  language = {en}
}

@book{cliffOrdinalMethodsBehavioral1996,
  title = {Ordinal {{Methods}} for {{Behavioral Data Analysis}}},
  author = {Cliff, Norman},
  year = {1996},
  publisher = {{Psychology Press}},
  address = {{New York, NY}},
  doi = {10.4324/9781315806730},
  abstract = {This book was written with the belief that ordinal statistical methods--sometimes discussed under the title of "nonparametric statistics"--deserve much more},
  file = {G\:\\Mi unidad\\Zotero\\undefined\\Cliff_2014_Ordinal Methods for Behavioral Data Analysis.pdf;D\:\\Zotero Library\\storage\\2CPS79U7\\9781315806730.html},
  isbn = {978-1-315-80673-0},
  language = {en}
}

@article{cohenPowerPrimer1992,
  title = {A Power Primer},
  author = {Cohen, Jacob},
  editor = {Kazdin, A},
  year = {1992},
  volume = {112},
  pages = {155--159},
  publisher = {{American Psychological Association}},
  issn = {00332909},
  doi = {10.1037/0033-2909.112.1.155},
  abstract = {One possible reason for the continued neglect of statistical power analysis in research in the behavioral sciences is the inaccessibility of or difficulty with the standard material. A convenient, although not comprehensive, presentation of required sample sizes is provided here. Effect-size indexes and conventional values for these are given for operationally defined small, medium, and large effects. The sample sizes necessary for .80 power to detect effects at these levels are tabled for eight standard statistical tests: (a) the difference between independent means, (b) the significance of a product-moment correlation, (c) the difference between independent rs, (d) the sign test, (e) the difference between independent proportions, (f) chi-square tests for goodness of fit and contingency tables, (g) one-way analysis of variance, and (h) the significance of a multiple or multiple partial correlation.},
  file = {G\:\\Mi unidad\\Papers\\Psychological Bulletin\\Cohen, 1992, A power primer.pdf},
  isbn = {1557989583},
  journal = {Psychological Bulletin},
  keywords = {Effect Size,Statistical Power},
  number = {1},
  pmid = {19565683}
}

@book{cohenStatisticalPowerAnalysis1988,
  title = {Statistical Power Analysis for the Behavioral Sciences},
  author = {Cohen, Jacob},
  year = {1988},
  edition = {2nd ed.},
  publisher = {{Erlbaum}},
  address = {{Hillsdale, NJ}},
  isbn = {978-0-12-179060-8}
}

@misc{correaScriptsVideo2020,
  title = {Scripts En {{R}} [{{Video}}]},
  author = {Correa, Juan Carlos},
  year = {2020},
  month = apr,
  abstract = {Un script en R es un archivo de texto plano donde se escriben las {\'o}rdenes que la computadora obedece para sacar c{\'a}lculos, construir bases de datos, hacer gr{\'a}ficos estad{\'i}sticos, o cualquier otra manipulaci{\'o}n de objetos.},
  howpublished = {https://www.youtube.com/watch?v=ejQ0BS2gVJI},
  journal = {YouTube},
  note = {http://web.archive.org/web/20200419181836/https://www.youtube.com/watch?v=ejQ0BS2gVJI}
}

@article{correllAvoidCohenSmall2020,
  title = {Avoid {{Cohen}}'s `{{Small}}', `{{Medium}}', and `{{Large}}' for {{Power Analysis}}},
  author = {Correll, Joshua and Mellinger, Christopher and McClelland, Gary H. and Judd, Charles M.},
  year = {2020},
  month = mar,
  volume = {24},
  pages = {200--207},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2019.12.009},
  abstract = {One of the most difficult and important decisions in power analysis involves specifying an effect size. Researchers frequently employ definitions of small, medium, and large that were proposed by Jacob Cohen. These definitions are problematic for two reasons. First, they are arbitrary, based on non-scientific criteria. Second, they are inconsistent, changing dramatically and illogically as a function of the statistical test a researcher plans to use (e.g., t-test versus regression). These problems may be unknown to many researchers, but they have a huge impact on power analyses. Estimates of the required n may be inappropriately doubled or cut in half. For power analyses to have any meaning, these definitions of effect size should be avoided.},
  file = {G\:\\Mi unidad\\Zotero\\Trends in Cognitive Sciences\\Correll et al_2020_Avoid Cohen’s ‘Small’, ‘Medium’, and ‘Large’ for Power Analysis.pdf;D\:\\Zotero Library\\storage\\XAGDR5BI\\S1364661319302979.html},
  journal = {Trends in Cognitive Sciences},
  keywords = {effect size,research design,research methods},
  language = {en},
  number = {3}
}

@article{CrisisReplicacion2020,
  title = {{Crisis de replicaci{\'o}n}},
  year = {2020},
  month = jan,
  abstract = {La crisis de replicaci{\'o}n (o crisis de replicabilidad) refiere a una crisis metodol{\'o}gica en las ciencias en que investigadores han encontrado que los resultados de muchos de los experimentos cient{\'i}ficos son dif{\'i}ciles o imposibles de replicar en investigaciones posteriores por investigadores independientes o por los investigadores originales de estos estudios.[1]\mbox{} Aunque la crisis ha estado presente por largo tiempo, la frase fue acu{\~n}ada a inicios de la d{\'e}cada de 2010 como parte de una mayor concienciaci{\'o}n sobre el problema. La crisis de replicaci{\'o}n representa un tema de investigaci{\'o}n importante en el campo de la metaciencia.
Considerando que la reproducibilidad de experimentos es una parte esencial del m{\'e}todo cient{\'i}fico, la incapacidad de replicar los estudios de otros tiene consecuencias potencialmente graves para muchos campos de la ciencia, en los que teor{\'i}as significativas son hundidas o irreproducibles en su trabajo experimental.
La crisis de replicaci{\'o}n ha sido particularmente discutida en psicolog{\'i}a (y en particular, psicolog{\'i}a social) y en medicina, donde un n{\'u}mero de esfuerzos ha sido hecho para re-investigar resultados cl{\'a}sicos, con el fin de determinar tanto la fiabilidad de los resultados, y, si se hallan inv{\'a}lidos, las razones para el fracaso de la replicaci{\'o}n.[2]\mbox{}[3]\mbox{}},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {D\:\\Zotero Library\\storage\\LTYCFMFU\\index.html},
  journal = {Wikipedia, la enciclopedia libre},
  language = {es},
  note = {Page Version ID: 123122916; http://web.archive.org/web/20200413085536/https://es.wikipedia.org/w/index.php?title=Crisis\_de\_replicaci\%C3\%B3n\&oldid=123122916}
}

@article{faulPowerFlexibleStatistical2007,
  title = {G*{{Power}} 3: {{A}} Flexible Statistical Power Analysis Program for the Social, Behavioral, and Biomedical Sciences},
  author = {Faul, Franz and Erdfelder, Edgar and Lang, Albert-Georg and Buchner, Axel},
  year = {2007},
  month = may,
  volume = {39},
  pages = {175--191},
  issn = {1554-351X},
  doi = {10.3758/BF03193146},
  abstract = {G*Power (Erdfelder, Faul, \& Buchner, 1996) was designed as a general stand-alone power analysis program for statistical tests commonly used in social and behavioral research. G*Power 3 is a major extension of, and improvement over, the previous versions. It runs on widely used computer platforms (i.e., Windows XP, Win- dows Vista, and Mac OS X 10.4) and covers many different statistical tests of the t, F, and ?2 test families. In addition, it includes power analyses for z tests and some exact tests. G*Power 3 provides improved effect size calculators and graphic options, supports both distribution-based and design-based input modes, and offers all types of power analyses in which users might be interested. Like its predecessors, G*Power 3 is free.},
  archivePrefix = {arXiv},
  eprint = {1011.1669v3},
  eprinttype = {arxiv},
  file = {G\:\\Mi unidad\\Papers\\Behavior Research Methods\\Faul et al., 2007, GPower 3 A flexible statistical power analysis program for the social, behavioral, and biomedical sciences.pdf},
  isbn = {1554-351X},
  journal = {Behavior Research Methods},
  keywords = {power analysis,statistical power},
  number = {2},
  pmid = {17695343}
}

@article{faulStatisticalPowerAnalyses2009,
  title = {Statistical Power Analyses Using {{G}}*{{Power}} 3.1: {{Tests}} for Correlation and Regression Analyses},
  shorttitle = {Statistical Power Analyses Using {{G}}*{{Power}} 3.1},
  author = {Faul, Franz and Erdfelder, Edgar and Buchner, Axel and Lang, Albert-Georg},
  year = {2009},
  month = nov,
  volume = {41},
  pages = {1149--1160},
  issn = {1554-3528},
  doi = {10.3758/BRM.41.4.1149},
  abstract = {G*Power is a free power analysis program for a variety of statistical tests. We present extensions and improvements of the version introduced by Faul, Erdfelder, Lang, and Buchner (2007) in the domain of correlation and regression analyses. In the new version, we have added procedures to analyze the power of tests based on (1) single-sample tetrachoric correlations, (2) comparisons of dependent correlations, (3) bivariate linear regression, (4) multiple linear regression based on the random predictor model, (5) logistic regression, and (6) Poisson regression. We describe these new features and provide a brief introduction to their scope and handling.},
  file = {G\:\\Mi unidad\\Zotero\\Behavior Research Methods\\Faul et al_2009_Statistical power analyses using GPower 3.pdf},
  journal = {Behavior Research Methods},
  language = {en},
  number = {4}
}

@article{goedhartCalculationDistributionFree2016,
  title = {Calculation of a Distribution Free Estimate of Effect Size and Confidence Intervals Using {{VBA}}/{{Excel}}},
  author = {Goedhart, Joachim},
  year = {2016},
  month = sep,
  pages = {073999},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/073999},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}Reporting effect sizes aids the transparent presentation and independent interpretation of scientific data. However, calculation and reporting of effect sizes for data obtained in basic research is rare. A standardized effect size was reported by Norman Cliff, known as Cliff9s delta. It has several advantageous features, as (i) it makes no assumption on the shape of the underlying distribution, (ii) it works well for small to moderate samples (n\&gt;10), (iii) it is easy to calculate, and (iv) its basis is readily understood by non statisticians. Here, a VBA macro, implemented in Excel, is presented. The macro takes two independent samples as input and calculates Cliff9s delta with 95\% confidence intervals. The macro will reduce the barrier for calculating the effect size and can be a valuable tool for research and teaching.{$<$}/p{$>$}},
  copyright = {\textcopyright{} 2016, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  file = {G\:\\Mi unidad\\Zotero\\bioRxiv\\Goedhart_2016_Calculation of a distribution free estimate of effect size and confidence.pdf;D\:\\Zotero Library\\storage\\2WJJJYL7\\073999v2.html},
  journal = {bioRxiv},
  language = {en}
}

@article{holmSimpleSequentiallyRejective1979a,
  title = {A {{Simple Sequentially Rejective Multiple Test Procedure}}},
  author = {Holm, Sture},
  year = {1979},
  volume = {6},
  pages = {65--70},
  publisher = {{[Board of the Foundation of the Scandinavian Journal of Statistics, Wiley]; http://web.archive.org/web/20200420055213/https://www.jstor.org/stable/4615733}},
  issn = {0303-6898},
  abstract = {This paper presents a simple and widely applicable multiple test procedure of the sequentially rejective type, i.e. hypotheses are rejected one at a time until no further rejections can be done. It is shown that the test has a prescribed level of significance protection against error of the first kind for any combination of true hypotheses. The power properties of the test and a number of possible applications are also discussed.},
  journal = {Scandinavian Journal of Statistics},
  keywords = {⛔ No DOI found},
  number = {2}
}

@article{hubertyGroupOverlapBasis2000,
  title = {Group {{Overlap}} as a {{Basis}} for {{Effect Size}}},
  shorttitle = {Group {{Overlap}} as a {{Basis}} for {{Effect Size}}},
  author = {Huberty, Carl J. and Lowman, Laureen L.},
  year = {2000},
  volume = {60},
  pages = {543--563},
  publisher = {{Sage PublicationsSage CA: Thousand Oaks, CA; http://web.archive.org/web/20200413075616/https://journals.sagepub.com/doi/10.1177/0013164400604004}},
  doi = {10.1177/0013164400604004},
  abstract = {The research content of interest herein is that of comparison of means. It is generally recognized that statistical test p values do not adequately reflect mean...},
  file = {D\:\\Zotero Library\\storage\\MY67VRPU\\0013164400604004.html},
  journal = {Educational and Psychological Measurement},
  language = {en},
  number = {4}
}

@article{kelleyEffectSize2012,
  title = {On Effect Size},
  author = {Kelley, Ken and Preacher, Kristopher J.},
  year = {2012},
  volume = {17},
  pages = {137--152},
  issn = {1939-1463},
  doi = {10.1037/a0028086},
  journal = {Psychological Methods},
  number = {2}
}

@article{krzywinskiPointsSignificanceSignificance2013,
  title = {Points of Significance: {{Significance}}, {{P}} Values and t-Tests},
  author = {Krzywinski, Martin and Altman, Naomi},
  year = {2013},
  volume = {10},
  pages = {1041--1042},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7091},
  doi = {10.1038/nmeth.2698},
  abstract = {The P value reported by tests is a probabilistic significance, not a biological one.},
  file = {G\:\\Mi unidad\\Papers\\Nature Methods\\Krzywinski, Altman, 2013, Points of significance Significance, P values and t-tests.pdf},
  isbn = {1548-7091\textbackslash{}r1548-7105},
  journal = {Nature Methods},
  number = {11},
  pmid = {24344377}
}

@article{lakensEquivalenceTestingPsychological2018a,
  title = {Equivalence {{Testing}} for {{Psychological Research}}: {{A Tutorial}}},
  shorttitle = {Equivalence {{Testing}} for {{Psychological Research}}},
  author = {Lakens, Dani{\"e}l and Scheel, Anne M. and Isager, Peder M.},
  year = {2018},
  month = jun,
  volume = {1},
  pages = {259--269},
  publisher = {{SAGE Publications Inc; http://web.archive.org/web/20200414002605/https://journals.sagepub.com/doi/10.1177/2515245918770963}},
  issn = {2515-2459},
  doi = {10.1177/2515245918770963},
  abstract = {Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.},
  file = {G\:\\Mi unidad\\Zotero\\Advances in Methods and Practices in Psychological Science\\Lakens et al_2018_Equivalence Testing for Psychological Research.pdf},
  journal = {Advances in Methods and Practices in Psychological Science},
  language = {en},
  number = {2}
}

@article{lakensEquivalenceTestsPractical2017,
  title = {Equivalence {{Tests}}: {{A Practical Primer}} for t {{Tests}}, {{Correlations}}, and {{Meta}}-{{Analyses}}},
  shorttitle = {Equivalence {{Tests}}},
  author = {Lakens, Dani{\"e}l},
  year = {2017},
  month = may,
  publisher = {{SAGE PublicationsSage CA: Los Angeles, CA; http://web.archive.org/web/20200414002856/https://journals.sagepub.com/doi/10.1177/1948550617697177}},
  doi = {10.1177/1948550617697177},
  abstract = {Scientists should be able to provide support for the absence of a meaningful effect. Currently, researchers often incorrectly conclude an effect is absent based...},
  file = {G\:\\Mi unidad\\Zotero\\Social Psychological and Personality Science\\Lakens_2017_Equivalence Tests.pdf;D\:\\Zotero Library\\storage\\955WMPK5\\1948550617697177.html},
  journal = {Social Psychological and Personality Science},
  language = {en}
}

@misc{lakensIntroductionSuperpower2020,
  title = {Introduction to {{Superpower}}},
  author = {Lakens, Dani{\"e}l and Caldwell, Aaron R.},
  year = {2020},
  file = {D\:\\Zotero Library\\storage\\7I562MIB\\intro_to_superpower.html},
  howpublished = {http://shorturl.at/fnDX6},
  journal = {The Comprehensive R Archive Network},
  note = {http://web.archive.org/web/20200412065008/https://cran.r-project.org/web/packages/Superpower/vignettes/intro\_to\_superpower.html; http://web.archive.org/web/20200412065014/https://cran.r-project.org/web/packages/Superpower/vignettes/intro\_to\_superpower.html}
}

@article{lakensJustifyYourAlpha2018,
  title = {Justify Your Alpha},
  author = {Lakens, Dani{\"e}l and Adolfi, Federico G. and Albers, Casper J. and Anvari, Farid and Apps, Matthew A. J. and Argamon, Shlomo E. and Baguley, Thom and Becker, Raymond B. and Benning, Stephen D. and Bradford, Daniel E. and Buchanan, Erin M. and Caldwell, Aaron R. and Van Calster, Ben and Carlsson, Rickard and Chen, Sau-Chin and Chung, Bryan and Colling, Lincoln J. and Collins, Gary S. and Crook, Zander and Cross, Emily S. and Daniels, Sameera and Danielsson, Henrik and DeBruine, Lisa and Dunleavy, Daniel J. and Earp, Brian D. and Feist, Michele I. and Ferrell, Jason D. and Field, James G. and Fox, Nicholas W. and Friesen, Amanda and Gomes, Caio and {Gonzalez-Marquez}, Monica and Grange, James A. and Grieve, Andrew P. and Guggenberger, Robert and Grist, James and {van Harmelen}, Anne-Laura and Hasselman, Fred and Hochard, Kevin D. and Hoffarth, Mark R. and Holmes, Nicholas P. and Ingre, Michael and Isager, Peder M. and Isotalus, Hanna K. and Johansson, Christer and Juszczyk, Konrad and Kenny, David A. and Khalil, Ahmed A. and Konat, Barbara and Lao, Junpeng and Larsen, Erik Gahner and Lodder, Gerine M. A. and Lukavsk{\'y}, Ji{\v r}{\'i} and Madan, Christopher R. and Manheim, David and Martin, Stephen R. and Martin, Andrea E. and Mayo, Deborah G. and McCarthy, Randy J. and McConway, Kevin and McFarland, Colin and Nio, Amanda Q. X. and Nilsonne, Gustav and {de Oliveira}, Cilene Lino and {de Xivry}, Jean-Jacques Orban and Parsons, Sam and Pfuhl, Gerit and Quinn, Kimberly A. and Sakon, John J. and Saribay, S. Adil and Schneider, Iris K. and Selvaraju, Manojkumar and Sjoerds, Zsuzsika and Smith, Samuel G. and Smits, Tim and Spies, Jeffrey R. and Sreekumar, Vishnu and Steltenpohl, Crystal N. and Stenhouse, Neil and {\'S}wi{\k{a}}tkowski, Wojciech and Vadillo, Miguel A. and Van Assen, Marcel A. L. M. and Williams, Matt N. and Williams, Samantha E. and Williams, Donald R. and Yarkoni, Tal and Ziano, Ignazio and Zwaan, Rolf A.},
  year = {2018},
  month = mar,
  volume = {2},
  pages = {168--171},
  publisher = {{Nature Publishing Group; http://web.archive.org/web/20200411231609/https://www.nature.com/articles/s41562-018-0311-x}},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0311-x},
  abstract = {In response to recommendations to redefine statistical significance to P {$\leq$} 0.005, we propose that researchers should transparently report and justify all choices they make when designing a study, including the alpha level.},
  copyright = {2018 The Publisher},
  file = {G\:\\Mi unidad\\Zotero\\Nature Human Behaviour\\Lakens et al_2018_Justify your alpha.pdf;D\:\\Zotero Library\\storage\\N3XEDRXA\\s41562-018-0311-x.html},
  journal = {Nature Human Behaviour},
  language = {en},
  number = {3}
}

@article{lakensPerformingHighpoweredStudies2014,
  title = {Performing High-Powered Studies Efficiently with Sequential Analyses},
  author = {Lakens, Dani{\"e}l},
  year = {2014},
  month = dec,
  volume = {44},
  pages = {701--710},
  issn = {00462772},
  doi = {10.1002/ejsp.2023},
  file = {G\:\\Mi unidad\\Papers\\European Journal of Social Psychology\\Lakens, 2014, Performing high-powered studies efficiently with sequential analyses.pdf},
  journal = {European Journal of Social Psychology},
  number = {7}
}

@article{leongomezAnalisisPoderEstadistico2020,
  title = {An{\'a}lisis de Poder Estad{\'i}stico y C{\'a}lculo de Tama{\~n}o de Muestra En {{R}}},
  author = {Leong{\'o}mez, Juan David},
  year = {2020},
  month = apr,
  publisher = {{OSF; http://web.archive.org/web/20200420035751/https://osf.io/3qx6a/}},
  doi = {10.17605/OSF.IO/3QX6A},
  abstract = {Opciones gratuitas y abiertas, con {\'e}nfasis en los paquetes \{pwr\} y \{Superpower\} para R 
    Hosted on the Open Science Framework},
  file = {D\:\\Zotero Library\\storage\\886H3AYC\\3qx6a.html},
  language = {en}
}

@article{lokenMeasurementErrorReplication2017,
  title = {Measurement Error and the Replication Crisis},
  author = {Loken, Eric and Gelman, Andrew},
  year = {2017},
  month = feb,
  volume = {355},
  pages = {584--585},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aal3618},
  abstract = {Measurement error adds noise to predictions, increases uncertainty in parameter estimates, and makes it more difficult to discover new phenomena or to distinguish among competing theories. A common view is that any study finding an effect under noisy conditions provides evidence that the underlying effect is particularly strong and robust. Yet, statistical significance conveys very little information when measurements are noisy. In noisy research settings, poor measurement can contribute to exaggerated estimates of effect size. This problem and related misunderstandings are key components in a feedback loop that perpetuates the replication crisis in science.
The assumption that measurement error always reduces effect sizes is false
The assumption that measurement error always reduces effect sizes is false},
  copyright = {Copyright \textcopyright{} 2017, American Association for the Advancement of Science},
  file = {D\:\\Zotero Library\\storage\\NNXJHNNQ\\584.html},
  journal = {Science},
  language = {en},
  number = {6325},
  pmid = {28183939; http://web.archive.org/web/20200413090456/https://science.sciencemag.org/content/355/6325/584}
}

@article{macbethCliffDeltaCalculator2011,
  title = {{{Cliff}}\textasciiacute{}s {{Delta Calculator}}: {{A}} Non-Parametric Effect Size Program for Two Groups of Observations},
  shorttitle = {{{Cliff}}\textasciiacute{}s {{Delta Calculator}}},
  author = {Macbeth, Guillermo and Razumiejczyk, Eugenia and Ledesma, Rub{\'e}n Daniel},
  year = {2011},
  volume = {10},
  pages = {545--555},
  issn = {2011-2777},
  doi = {10.11144/Javeriana.upsy10-2.cdcp},
  copyright = {Derechos de autor},
  file = {G\:\\Mi unidad\\Zotero\\Universitas Psychologica\\Macbeth et al_2011_Cliff´s Delta Calculator.pdf;D\:\\Zotero Library\\storage\\VU2YY44B\\643.html},
  journal = {Universitas Psychologica},
  keywords = {aplicaciones psicológicas,delta de cliff,estadística no paramétrica,programa estadístico,tamaño del efecto},
  language = {en},
  number = {2}
}

@article{olejnikGeneralizedEtaOmega2003,
  title = {Generalized {{Eta}} and {{Omega Squared Statistics}}: {{Measures}} of {{Effect Size}} for {{Some Common Research Designs}}.},
  author = {Olejnik, Stephen and Algina, James},
  year = {2003},
  volume = {8},
  pages = {434--447},
  issn = {1939-1463},
  doi = {10.1037/1082-989X.8.4.434},
  journal = {Psychological Methods},
  number = {4}
}

@misc{quintanaNontechnicalGuidePerforming2019,
  title = {A Non-Technical Guide to Performing Power Analysis in {{R}} [{{Video}}]},
  author = {Quintana, Daniel S.},
  year = {2019},
  month = may,
  file = {D\:\\Zotero Library\\storage\\SZ76WNTW\\watch.html},
  howpublished = {https://youtu.be/ZIjOG8LTTh8},
  journal = {YouTube},
  note = {http://web.archive.org/web/20200414031440/https://www.youtube.com/watch?v=ZIjOG8LTTh8}
}

@article{quintanaStatisticalConsiderationsReporting2017,
  title = {Statistical Considerations for Reporting and Planning Heart Rate Variability Case-Control Studies},
  author = {Quintana, Daniel S.},
  year = {2017},
  volume = {54},
  pages = {344--349},
  issn = {1469-8986},
  doi = {10.1111/psyp.12798},
  abstract = {The calculation of heart rate variability (HRV) is a popular tool used to investigate differences in cardiac autonomic control between population samples. When interpreting effect sizes to quantify the magnitude of group differences, researchers typically use Cohen's guidelines of small (0.2), medium (0.5), and large (0.8) effects. However, these guidelines were originally proposed as a fallback for when the effect size distribution (ESD) was unknown. Despite the availability of effect sizes from hundreds of HRV studies, researchers still largely rely on Cohen's guidelines to interpret effect sizes and to perform power analyses to calculate required sample sizes for future research. This article describes an ESD analysis of 297 HRV effect sizes from between-group/case-control studies, revealing that the 25th, 50th, and 75th effect size percentiles correspond with effect sizes of 0.26, 0.51, and 0.88, respectively. The analyses suggest that Cohen's guidelines may underestimate the magnitude of small and large effect sizes and that HRV studies are generally underpowered. Therefore, to better reflect the observed ESD, effect sizes of 0.25, 0.5, and 0.9 should be interpreted as small, medium, and large effects (after rounding to the closest 0.05). Based on power calculations using the ESD, suggested sample sizes are also provided for planning suitably powered studies that are more likely to replicate. Researchers are encouraged to use the ESD data set or their own collected data sets in tandem with the provided analysis script to perform custom ESD and power analyses relevant to their specific research area.},
  copyright = {\textcopyright{} 2016 Society for Psychophysiological Research},
  file = {D\:\\Zotero Library\\storage\\JY3KFUNC\\psyp.html},
  journal = {Psychophysiology},
  keywords = {Effect size,Heart rate variability,Sample size,Statistical power},
  language = {en},
  note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/psyp.12798},
  number = {3}
}

@article{selyaPracticalGuideCalculating2012,
  title = {A Practical Guide to Calculating {{Cohen}}'s F{\textsuperscript{2}}, a Measure of Local Effect Size, from {{PROC MIXED}}},
  author = {Selya, Arielle S. and Rose, Jennifer S. and Dierker, Lisa C. and Hedeker, Donald and Mermelstein, Robin J.},
  year = {2012},
  volume = {3},
  pages = {111},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2012.00111},
  abstract = {Reporting effect sizes in scientific articles is increasingly widespread and encouraged by journals; however, choosing an effect size for analyses such as mixed-effects regression modeling and hierarchical linear modeling can be difficult. One relatively uncommon, but very informative, standardized measure of effect size is Cohen's f2, which allows an evaluation of local effect size, i.e. one variable's effect size within the context of a multivariate regression model. Unfortunately, this measure is often not readily accessible from commonly used software for repeated-measures or hierarchical data analysis. In this guide, we illustrate how to extract Cohen's f2 for two variables within a mixed-effects regression model using PROC MIXED in SAS \&\#174; software. Two examples of calculating Cohen's f2 for different research questions are shown, using data from a longitudinal cohort study of smoking development in adolescents. This tutorial is designed to facilitate the calculation and reporting of effect sizes for single variables within mixed-effects multiple regression models, and is relevant for analysis of repeated-measures or hierarchical/multilevel data that are common in experimental psychology, observational research, and clinical or intervention studies.},
  file = {G\:\\Mi unidad\\Zotero\\Frontiers in Psychology\\Selya et al_2012_A Practical Guide to Calculating Cohen’s f2, a Measure of Local Effect Size,.pdf},
  journal = {Frontiers in Psychology},
  keywords = {Cohen’s f2,effect size,hierarchical linear modeling,mixed-effects regression,PROC MIXED,R2,SAS},
  language = {English}
}

@article{streinerBestOftforgottenPractices2015,
  title = {Best (but Oft-Forgotten) Practices: The Multiple Problems of Multiplicity\textemdash{}Whether and How to Correct for Many Statistical Tests},
  shorttitle = {Best (but Oft-Forgotten) Practices},
  author = {Streiner, David L.},
  year = {2015},
  month = oct,
  volume = {102},
  pages = {721--728},
  publisher = {{Oxford Academic; http://web.archive.org/web/20200423063906/https://academic.oup.com/ajcn/article/102/4/721/4564678}},
  issn = {0002-9165},
  doi = {10.3945/ajcn.115.113548},
  abstract = {ABSTRACT.  Testing many null hypotheses in a single study results in an increased probability of detecting a significant finding just by chance (the problem of},
  file = {G\:\\Mi unidad\\Zotero\\The American Journal of Clinical Nutrition\\Streiner_2015_Best (but oft-forgotten) practices.pdf;D\:\\Zotero Library\\storage\\UIRKWEBK\\4564678.html},
  journal = {The American Journal of Clinical Nutrition},
  language = {en},
  number = {4}
}

@misc{zaiontzCliffDeltaReal2014,
  title = {Cliff's {{Delta}} | {{Real Statistics Using Excel}}},
  author = {Zaiontz, Charles},
  year = {2014},
  abstract = {Describes how to calculate Cliff's Delta non-parametric effect size in Excel.},
  file = {D\:\\Zotero Library\\storage\\G9JILQJG\\cliffs-delta.html},
  journal = {Real Statistics Using Excel},
  language = {en-US}
}


